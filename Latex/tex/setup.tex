\section{Experiment Setup}
\label{sec:setup}

Our experimentation was split in two parts: the first was about recognizing a single chord from a single chord track, the second one was about recognizing the chords of full songs.\\
%
For the first part, we used the dataset \texttt{jim2012Chords} \cite{jim2012Chords} created for their paper \cite{JimChordsPaper}. It's composed of a total of over $2.000$ recordings of 10 guitar chords (both major and minor triads). Four different Guitars are used, as well as Piano, Violin and Accordion. Some tracks were recorded in an anechoic chamber, some other in a noisy environment. For each one of the tracks we obtain a 12-dimensional feature vector using \textit{Chroma Toolbox} and taking the maximum value for each note. Looking at Fig.~\ref{fig:CENSexample}, the idea is that we are interested in high values of specific groups of notes in order to guess the chord and the maximum value is a simple way of doing this. It's biggest disadvantage is that impulsive noisy environment can easily mask the useful information introducing high values in the wrong notes. Finally, we run the different methods on these obtained 12D vectors.\\
%
For the second part we used as dataset \textit{The Beatles}' discography, which has been professionally transcribed by Christopher Harte. This dataset contains the chords transcription of all The Betales's songs. Harte's work has proven to be extremely high quality and is an excellent resource for speech processing research. Details on how the chords transcription was achieved can be found in \cite{HartePaper} and in \cite{HarteThesis}. In his work Harte performed a very detailed transcription, obtaining a wide set of chords. For reduce computational cost we mapped the labels transcripted by Harte into $24$ labels, which coincide with the $12$ notes of the chromatic scale in the major and minor version. For similar reason we selected only $150$ Beatles songs out the $180$ trasncribed by Harte, keeping anyway a big number of songs to work on. From now on, we indicate our labels dataset with $\Lambda$ and our songs dataset with $\mathcal{D}_{Beatles}$. \\
%
The second part of the experiment was divided in three steps which are \textit{pre-processing}, \textit{training} and \textit{testing}. As machine learning approach, we randomly divided our dataset in two subset that we call \textit{training dataset} $\mathcal{D}_{train}$ and \textit{testing dataset}  $\mathcal{D}_{test}$. We call $r_{train}$ the ratio between $|\mathcal{D}_{train}|$ and $|\mathcal{D}_{Beatles}|$ and $r_{test}$ the ratio between $|\mathcal{D}_{train}|$ and $|\mathcal{D}_{Beatles}|$. During the experiment we varied the value of $r_{test} \in \{0.1,0.2,0.3\}$, consequently obtaining $r_{train}$ as $1-r_{test}$. Every song of both the subsets was processed according to one of the technique offered by \textit{Chorma toolbox}. We noticed that we set toolbox parameters so that we didn't get an exaggerated number of frames per song. In \textit{results} section we will see that this choice could make outputs weaker, especially in CLP and CRP cases. After features extraction was completed, we obtained a sequence of \textit{frame-features} for each song in $\mathcal{D}_{Beatles}$; the length of the sequence depended both on the song duration and the chosen featuring process $(CENS,CLP,CRP)$. Harte's work gave us information about the chord periods in each songs; we wanted to make compatible this information with the data obtained by features extraction. With this purpose we divided the periods computed by Harte in sub-periods with equal time-length to the frames computed with Chroma toolbox. For each song we obtained a sequence of \textit{frame-labels} with equal length to the sequence of \textit{frame-features}; from now on we indicate the \textit{frame-labels} sequence for a generic song with $\mathcal{L}_{Harte}$. Finally we noticed that almost all the songs presented periods with no sounds; in the dataset these period were marked with the '$N$' label. Obviously '$N$' frames were out of our interested and therefore were discarded. \\
%
At the end of the \textit{pre-processing} phase we obtained two sets of songs $(\mathcal{D}_{train},\mathcal{D}_{test})$, in wich every songs is assigned to both a sequence of \textit{frame-features} and a sequence of \textit{frame-labels}. The data contained in $\mathcal{D}_{train}$ were then used to train the MC-SVM and subsequently build the HMM. MC-SVM training was achieved using one chord at time, as done in the first part of the experiment, and using as kernel the \textit{polynomial} one. HMM training was achieved testing the data contained in $\mathcal{D}_{train}$ with the MC-SVM previously built. The resulting errors of this process allowed us to establish values for the \textit{emission probabilities}. Analyzing directly all the $\mathcal{L}_{Harte}$ \textit{transition probabilities} and \textit{initial probabilities} values were computed. \\
%
Once all our methods were trained, the \textit{testing} began. We took the \textit{frame-features} of $\mathcal{D}_{test}$ and we processed them first only with MC-SVM methods, then also with HMM. Therefore for each song we obtained two different outputs, that we call $\mathcal{L}_{SVM}$ and $\mathcal{L}_{HMM}$. $\mathcal{L}_{SVM}$ corresponds to the \textit{frame-labels} sequence produced by the MC-SVM. In this process every frame was assigned to a chord, without taking in account the others frame componing the song.  $\mathcal{L}_{HMM}$ corresponds to the \textit{frame-labels} sequence produced by the HMM, which took MC-SVM's output and tried to improve it. The HMM's output was obtained implementing the Viterbi algorithm, which allowed to taking into account the whole sequence of chords that compone the song.
