\section{Experiment Setup}
\label{sec:setup}

Our experimentation was split in two parts: the first was about recognizing a single chord from a single chord track, the second one was about recognizing the chords of full songs.\\
%
For the first part, we used the dataset \texttt{jim2012Chords} \cite{jim2012Chords} created for their paper \cite{JimChordsPaper}. It's composed of a total of over $2.000$ recordings of 10 guitar chords (both major and minor triads). Four different Guitars are used, as well as Piano, Violin and Accordion. Some tracks were recorded in an anechoic chamber, some other in a noisy environment. For each one of the tracks we obtain a 12-dimensional feature vector using \textit{Chroma Toolbox} and taking the maximum value for each note. Looking at Fig.~\ref{fig:CENSexample}, the idea is that we are interested in high values of specific groups of notes in order to guess the chord and the maximum value is a simple way of doing this. It's biggest disadvantage is that impulsive noisy environment can easily mask the useful information introducing high values in the wrong notes. Finally, we run the different methods on these obtained 12D vectors.\\
%
The second part of our work is revealed much more complex. We used as dataset \textit{The Beatles}'s' discography, which has been professionally transcribed by Christopher Harte. This dataset contains the chords transcription of all The Betales's songs. Harte's work has proven to be is extremely high quality and therefore is an excellent resource for speech processing research. Details on how the chords transcription was achieved can be found in \cite{HartePaper} and in \cite{HarteThesis}. We notice that in his work Harte performed a very detailed transcription, obtaining a very wide set of chords. For reduce computational cost we map the chord labels written by Harte into $24$ labels, which coincide with the $12$ traids in the major and minor version. For similar reason we selected only $150$ Beatles songs out the $180$ trasncribed by Harte, keeping anyway a big number of songs to work on. From now on, we indicate our songs dataset with $\mathcal{D}_{Beatles}$ and our labels dataset with $\Lambda$. \\
%
We can divide the second part of our experiment in three steps which are \textit{pre-processing}, \textit{training} and \textit{training}. First, as machine learning approach, we randomly divide our dataset in two subset that we call \textit{training dataset} $\mathcal{D}_{train}$ and \textit{testing dataset}  $\mathcal{D}_{test}$. We call $r_{train}$ the ratio between $|\mathcal{D}_{train}|$ and $|\mathcal{D}_{Beatles}|$ and $r_{test}$ the ratio between $|\mathcal{D}_{train}|$ and $|\mathcal{D}_{Beatles}|$. During the experimentation we varied the value of $r_{test} \in \{0.1,0.2,0.3\}$, consequently obtaining $r_{train}$ as $1-r_{test}$. Every song of both the subsets is processed according to one of the technique offered by \textit{Chorma toolbox}. We notice that we set toolbox parameters so that we do not get an exaggerated number of frames per song, containing the computational cost. In \textit{results} section we will see that this choice could make outputs weaker, especially in CLP and CRP case. After features extraction we obtain a list of frame-features; the length of the list depends both on the songs duration and the chosen featuring process $(CENS,CLP,CRP)$. Harte's work give us information about the chord intervals in each songs; we want to make compatible these information with the data obtained by features extraction. For this purpose we divide the original intervals made by Harte in sub-intervals with equal time-length to the frames computed with Chroma toolbox. What we obtained was a list of chord labels, with equal length to the list of frame-features; from now on we indicate this list with $\mathcal{L}_{Harte}$. Finally we notice that almost all the songs present periods with no sounds; in the dataset these period are marked with the '$N$' label. Obviously the frames marked with '$N$' were out of our interested and are discarded. \\
%
At the end of the previously \textit{pre-processing} phase we obtain two set of songs $(\mathcal{D}_{train},\mathcal{D}_{test})$, in wich every songs is assigned to both a sequence of \textit{frame-features} and a sequence of \textit{frame-labels}. The data contained in $\mathcal{D}_{train}$ are then used to train the MC-SVM and subsequently achieve the HMM. MC-SVM training is achieved using one chord at time, as done in the first part of the experiment. In the HMM realization we test the data contained in $\mathcal{D}_{train}$ with the MC-SVM previously built. In particular for each  the resulting errors allow us to establish values for the \textit{emission probabilities}. \textit{Transition probabilities} and \textit{initial probabilities} values are computed directly from the $\mathcal{D}_{train}$ labels sequences transcribed by Harte. \\
%
Once all our methods are trained, the \textit{testing} begins. We take the frame-features of $\mathcal{D}_{test}$ and we process them first only with MC-SVM methods, then also with HMM. Therefore for each song we obtain two different outputs, that we call $\mathcal{L}_{SVM}$ and $\mathcal{L}_{HMM}$. $\mathcal{L}_{SVM}$ one corresponds to the sequence of labels produced by the MC-SVM. In this process every frame is assigned to a chord, without taking in account the others frame componing the testing song. $\mathcal{L}_{HMM}$ corresponds to the sequence of labels produced by the HMM, which takes MC-SVM's output and tries to improve it. As previously said the HMM's output is obtained implementing the Viterbi algorithm, which search the most likely chords sequence starting from the observed chords. Proceding in this way, HMM allows to taking into account the whole sequence of chords that compone the song and therefore assuming a more complete vision of the problem.
