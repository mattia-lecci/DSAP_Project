\section{Experiment Setup}
\label{sec:setup}

Our experimentation was split in two parts: the first was about recognizing a single chord from a single chord track, the second one was about recognizing the chords of full songs.\\
%
For the first part, we used the dataset \texttt{jim2012Chords} \cite{jim2012Chords} created for their paper \cite{JimChordsPaper}. It's composed of a total of over $2.000$ recordings of 10 guitar chords (both major and minor triads). Four different Guitars are used, as well as Piano, Violin and Accordion. Some tracks were recorded in an anechoic chamber, some other in a noisy environment. For each one of the tracks we obtain a 12-dimensional feature vector using \textit{Chroma Toolbox} and taking the maximum value for each note. Looking at Fig.~\ref{fig:CENSexample}, the idea is that we are interested in high values of specific groups of notes in order to guess the chord and the maximum value is a simple way of doing this. It's biggest disadvantage is that impulsive noisy environment can easily mask the useful information introducing high values in the wrong notes. Finally, we run the different methods on these obtained 12D vectors.\\
%
For the second part we used as dataset \textit{The Beatles}'s' discography, which has been professionally transcribed by Christopher Harte. This dataset contains the chords transcription of all The Betales's songs. Harte's work has proven to be extremely high quality and is an excellent resource for speech processing research. Details on how the chords transcription was achieved can be found in \cite{HartePaper} and in \cite{HarteThesis}. In his work Harte performed a very detailed transcription, obtaining a wide set of chords. For reduce computational cost we mapped the labels transcripted by Harte into $24$ labels, which coincide with the $12$ notes of the chromatic scale in the major and minor version. For similar reason we selected only $150$ Beatles songs out the $180$ trasncribed by Harte, keeping anyway a big number of songs to work on. From now on, we indicate our songs dataset with $\mathcal{D}_{Beatles}$ and our labels dataset with $\Lambda$. \\
%
The second part of our experiment can be divided in three steps which are \textit{pre-processing}, \textit{training} and \textit{training}. As machine learning approach, we randomly divided our dataset in two subset that we call \textit{training dataset} $\mathcal{D}_{train}$ and \textit{testing dataset}  $\mathcal{D}_{test}$. We call $r_{train}$ the ratio between $|\mathcal{D}_{train}|$ and $|\mathcal{D}_{Beatles}|$ and $r_{test}$ the ratio between $|\mathcal{D}_{train}|$ and $|\mathcal{D}_{Beatles}|$. During the experiment we varied the value of $r_{test} \in \{0.1,0.2,0.3\}$, consequently obtaining $r_{train}$ as $1-r_{test}$. Every song of both the subsets was processed according to one of the technique offered by \textit{Chorma toolbox}. We noticed that we set toolbox parameters so that we didn't get an exaggerated number of frames per song. In \textit{results} section we will see that this choice could make outputs weaker, especially in CLP and CRP cases. After features extraction was completed, we obtained the sequences of \textit{frame-features} of all $\mathcal{D}_{Beatles}$ songs; the length of the sequences depended both on the song durations and the chosen featuring process $(CENS,CLP,CRP)$. Harte's work gave us information about the chord periods in each songs; we wanted to make compatible these information with the data obtained by features extraction. For this purpose we divided the periods computed by Harte in sub-periods with equal time-length to the frames computed with Chroma toolbox. What we obtained were sequences of \textit{frame-labels} with equal length to the sequences of \textit{frame-features}; from now on we indicate the \textit{frame-labels} sequence for a single song with $\mathcal{L}_{Harte}$. Finally we noticed that almost all the songs presented periods with no sounds; in the dataset these period were marked with the '$N$' label. Obviously the '$N$' frames were out of our interested and therefore were discarded. \\
%
At the end of the \textit{pre-processing} phase we obtained two sets of songs $(\mathcal{D}_{train},\mathcal{D}_{test})$, in wich every songs was assigned to both a sequence of \textit{frame-features} and a sequence of \textit{frame-labels}. The data contained in $\mathcal{D}_{train}$ were then used to train the MC-SVM and subsequently realize the HMM. MC-SVM training was achieved using one chord at time, as done in the first part of the experiment. HMM training was achieved testing the data contained in $\mathcal{D}_{train}$ with the MC-SVM previously built. The resulting errors of the test allowed us to establish values for the \textit{emission probabilities}. Instead \textit{Transition probabilities} and \textit{initial probabilities} values were computed directly looking at all the $\mathcal{L}_{Harte}$. \\
%
Once all our methods were trained, the \textit{testing} began. We took the \textit{frame-features} of $\mathcal{D}_{test}$ and we processed them first only with MC-SVM methods, then also with HMM. Therefore for each song we obtained two different outputs, that we call $\mathcal{L}_{SVM}$ and $\mathcal{L}_{HMM}$. $\mathcal{L}_{SVM}$ corresponds to the \textit{frame-labels} sequence produced by the MC-SVM. In this process every frame was assigned to a chord, without taking in account the others frame componing the song.  $\mathcal{L}_{HMM}$ corresponds to the \textit{frame-labels} sequence produced by the HMM, which took MC-SVM's output and tried to improve it. The HMM's output was obtained implementing the Viterbi algorithm, which searched the most likely \textit{frame-labels} sequence depending on the observed chords. Doing this, HMM allowed to taking into account the whole sequence of chords that compone the song and Ã¬assuming a more complete perspective of the problem.
