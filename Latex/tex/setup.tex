\section{Experiment Setup}
\label{sec:setup}

Our experimentation was split in two parts: the first was about recognizing a single chord from a single chord track, the second one was about recognizing the chords of full songs.\\
%
For the first part, we used the dataset \texttt{jim2012Chords} \cite{jim2012Chords} created for their paper \cite{JimChordsPaper}. It's composed of a total of over $2.000$ recordings of 10 guitar chords (both major and minor triads). Four different Guitars are used, as well as Piano, Violin and Accordion. Some tracks were recorded in an anechoic chamber, some other in a noisy environment. For each one of the tracks we obtain a 12-dimensional feature vector using \textit{Chroma Toolbox} and taking the maximum value for each note. Looking at Fig.~\ref{fig:CENSexample}, the idea is that we are interested in high values of specific groups of notes in order to guess the chord and the maximum value is a simple way of doing this. It's biggest disadvantage is that impulsive noisy environment can easily mask the useful information introducing high values in the wrong notes. Finally, we run the different methods on these obtained 12D vectors.\\
%
The second part of our work is revealed much more complex. As we will see, it can be divided in three phase which are \textit{pre-processing}, \textit{training} and \textit{training}. In all the three phases we used as dataset \textit{The Beatles}'s' discography, which has been professionally transcribed by Christopher Harte. It has been proven that Harte's work is extremely high quality and therefore is a excellent and reliable resource for speech processing research. Details on how the transcription was achieved can be found in \cite{HartePaper} and in \cite{HarteThesis}. We notice that Harted in his work performed a very detailed transcription, obtaining a very wide set of chords. For reduce computational cost we map the chord labels written by Harte into $24$ labels, which coincide with the $12$ traids in the major and minor version. For similar reason we selected only $150$ Beatles songs out the $180$ trasncribed by Harte, keeping anyway a big number of songs to work on. From now on, we indicate our songs dataset with $\mathcal{D}$. \\
%
As machine learning approach we randomly divide our dataset in two subset that we call \textit{training dataset} $\mathcal{D}_{train}$ and \textit{testing dataset}  $\mathcal{D}_{test}$, each containing a sequence of songs. We call $r_{train}$ the ratio between $|\mathcal{D}_{train}|$ and $|\mathcal{D}|$ and $r_{test}$ the ratio between $|\mathcal{D}_{train}|$ and $|\mathcal{D}|$. During the experimentation we varied the value of $r_{test}$ in the set $\{0.1,0.2,0.3\}$, consequently obtaining $r_{train}$ as $1-r_{test}$. Every song of both the subsets is processed according to one of the procedures offered by \textit{Chorma toolbox}. We notice that we set the parameters of the toolbox so that we do not get an exaggerated number of frames per song, containing the computational cost. This choice could make outputs weaker, especially in CLP and CRP case. After features extraction, songs result being composed by sequences of frames with a variable number, which depends both on $(r_{test},r_{train})$ and on the chosen featuring process $(CENS,CLP,CRP)$. Harte's work give us information about the chord intervals in each songs. We need to make homogeneous the two different results that we have. For this purpose we divide the intervals computed by Harte in sub-intervals with equal time-length to the frames computed with Chroma toolbox. Finally we notice that almost all the songs present periods with no sounds; in the dataset these period are marked with the '$N$' label. Frames related to these periods are obviously out of our interested and are consequently discarded. \\
%
At the end of the previously \textit{pre-processing} phase we obtain two set of songs $(\mathcal{D}_{train},\mathcal{D}_{test})$, in wich every songs is assigned to both a sequence of \textit{frame-features} and a sequence of \textit{frame-labels}. The data contained in $\mathcal{D}_{train}$ are then used to train the MC-SVM and subsequently achieve the HMM. In the HMM realization we test the data contained in $\mathcal{D}_{train}$ with the MC-SVM previously built. The resulting errors allow us to establish values for the \textit{emission probabilities}. \textit{Transition probabilities} and \textit{initial probabilities} values are computed directly from the labels sequences transcribed by Harte. \\
%
Once all our methods are trained, the \textit{testing} begins. We take the frame-features of $\mathcal{D}_{test}$ and we process them first only with MC-SVM methods, then also with HMM. Therefore for each song we obtain two different results. The first one corresponds to the sequence of labels produced by the MC-SVM. In this process every frame is assigned to a chord, without taking in account the others frame componing the testing song. The second result corresponds to the sequence of labels produced by the HMM, which takes MC-SVM's result and tries to improve it. As previously said the HMM's output is obtained implementing the Viterbi algorithm, which search the most likely chords sequence starting from the observed chords. Proceding in this way, HMM allows to taking the whole sequence of chords that compone the song and therefore assuming a more complete vision of the problem.
