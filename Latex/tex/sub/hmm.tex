\subsection{Hidden Markov Models (HMMs)}
\label{subsec:hmm}

\textit{Hidden Markov Models} are an extremely powerful technique that has been widely used in speech recognition application for twenty years. A HMM consistis in an extension of the classical Markov Model scenario, in which observations are considered probabilistic functions of states. The final result is a pair of stochastic processes connected to each other; one process is directly observable, the other process is not and therefore is called "hidden". A more detailed theory about HMM working can be found in \cite{LawrenceHMMtutorial}. \\
%
In speech recognition field, HMMs are used as statistical method in order to recover specific sequences of sound. The validity of HMMs in the the particular case of chords transcription has already been demostrated several times, as in \cite{AlexDanEMplusHMM} and \cite{belpickMusic}. Both these works utilize the HMM approach proposed in \cite{GoldMorganSpeechRecogn}, training HMM with the \textit{expectation-maximization algorithm}. As our work is not limited to implementing HMM, we choose to take a different approach. In particular we combined HMM to the SVM technique previously presented. \\
%
As suggested in \cite{GoldMorganSpeechRecogn}, HMM working can be supported by \textit{Viterbi algorithm}. Given a HMM scenario and a sequence of observables, Viterbi algorithm aims to find the most likely sequence of states. It works in a recursive mode, first finding the probability of each state for each observable, then identyfing the most probable states sequence. Given a total number of states $N$ and a sequence of observables $T$ long, the Viterbi algorithm shows a complexity equal to $O(T \times N^2)$. In our work the implementation of Viterbi algorithm represented one of the most difficult challenges. Codes already existing raised problems and therefore they needed to be carefully adapted to the experiment setup.
