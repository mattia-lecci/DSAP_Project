\subsection{Hidden Markov Models (HMMs)}
\label{subsec:hmm}

\gls{hmm} represents an extremely powerful technique and has been widely used in speech recognition application for twenty years. A \gls{hmm} consistis in an extension of the classical Markov Model scenario, in which observations are considered probabilistic functions of states. The final result is a pair of stochastic processes connected to each other; one process is directly observable, the other process is not and therefore is called "hidden". A more detailed theory about \gls{hmm} working can be found in \cite{LawrenceHMMtutorial}. \\
%
In the speech recognition field \gls{hmm}s are used as statistical method in order to recover specific sequences of sound. The validity of \gls{hmm}s in the the particular case of chord transcription has already been demostrated in numerous papers including \cite{AlexDanEMplusHMM} and \cite{belpickMusic}. Both these work utilize the \gls{hmm} approach proposed in \cite{GoldMorganSpeechRecogn}, training \gls{hmm} with the \textit{Expectation-Maximization Algorithm}. As this work is not limited to implementing \gls{hmm}, we choose to take a different approach. In particular we combine \gls{hmm} to the SVM technique previously presented. \\
%
As suggested in \cite{GoldMorganSpeechRecogn} the working of \gls{hmm} can be supported by the \textit{Viterbi Algorithm}. This algorithm aims to find the most likely sequence of states given the sequence of observables in a \gls{hmm} scenario. It works in a recursive mode, first finding the probability of each \gls{hmm} state for each observable, then identyfing the most probable states sequence. Given a total number of states equal to $N$ and a sequence of observables $T$ long, the Viterbi algorithm shows a complexity equal to $O(T \times N^2)$. The implementation of Vierbi algorithm represents the most difficult challenge in the \gls{hmm} approach. The codes already existing present problems and there fore need to be carefully adapted to our scenario. 
