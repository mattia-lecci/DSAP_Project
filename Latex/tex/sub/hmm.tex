\subsection{Hidden Markov Models (HMMs)}
\label{subsec:hmm}

\textit{Hidden Markov Models} are an extremely powerful technique that has been widely used in speech recognition application for twenty years. A HMM consistis in an extension of the classical Markov Model scenario, in which observations are considered probabilistic functions of states. The final result is a pair of stochastic processes connected to each other; one process is directly observable, the other process is not and therefore is called "hidden". A more detailed theory about HMM working can be found in \cite{LawrenceHMMtutorial}. \\
%
In the speech recognition field, HMMs are used as statistical method in order to recover specific sequences of sound. The validity of HMMs in the the particular case of chords transcription has already been demostrated several times, as in \cite{AlexDanEMplusHMM} and \cite{belpickMusic}. Both of these works use the HMM approach proposed in \cite{GoldMorganSpeechRecogn}, training HMM with the \textit{expectation-maximization algorithm}. As our work is not limited to implementing HMM, we decided to combine HMM with the SVM technique previously presented. \\
%
As suggested in \cite{GoldMorganSpeechRecogn}, the \textit{Viterbi algorithm} is an efficient way of working with HMMs. Given a HMM scenario and a sequence of observations, Viterbi's algorithm aims to find the most likely sequence of states. Given a total number of states $N$ and a sequence of $T$ observations, the Viterbi algorithm has a complexity equal to $O(TN^2)$. In our work the implementation of Viterbi's algorithm represented one of the most difficult challenges. Already existing code \cite{MatlabViterbi} raised problems and therefore it needed to be carefully adapted to the experiment setup.
