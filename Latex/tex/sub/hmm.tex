\subsection{Hidden Markov Models (HMMs)}
\label{subsec:hmm}

\textit{Hidden Markov Model} represents an extremely powerful technique that has been widely used in speech recognition application for twenty years. A HMM consistis in an extension of the classical Markov Model scenario, in which observations are considered probabilistic functions of states. The final result is a pair of stochastic processes connected to each other; one process is directly observable, the other process is not and therefore is called "hidden". A more detailed theory about HMM working can be found in \cite{LawrenceHMMtutorial}. \\
%
In the speech recognition field HMMs are used as statistical method in order to recover specific sequences of sound. The validity of HMMs in the the particular case of chord transcription has already been demostrated in numerous papers including \cite{AlexDanEMplusHMM} and \cite{belpickMusic}. Both these work utilize the HMM approach proposed in \cite{GoldMorganSpeechRecogn}, training HMM with the \textit{Expectation-Maximization Algorithm}. As this work is not limited to implementing HMM, we choose to take a different approach. In particular we combine HMM to the SVM technique previously presented. \\
%
As suggested in \cite{GoldMorganSpeechRecogn} the working of HMM can be supported by the \textit{Viterbi Algorithm}. This algorithm aims to find the most likely sequence of states given the sequence of observables in a HMM scenario. It works in a recursive mode, first finding the probability of each HMM state for each observable, then identyfing the most probable states sequence. Given a total number of states equal to $N$ and a sequence of observables $T$ long, the Viterbi algorithm shows a complexity equal to $O(T \times N^2)$. The implementation of Viterbi algorithm represents the most difficult challenge in the HMM approach. Codes already existing often raise problems and therefore need to be carefully adapted to our scenario.
